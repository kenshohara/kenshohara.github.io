<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width" initial-scale="1">
    <link rel="stylesheet" href="src/css/normalize.min.css">
    <link rel="stylesheet" href="src/css/milligram.min.css">
    <link rel="stylesheet" href="src/css/main.css">
    <title>Kensho Hara</title>
  </head>
  <body>
    <header>
      <ul class="nav-bar">
        <li class="item">Top</li>
        <li class="item">Profile</li>
        <li class="item">Research</li>
        <li class="item">Publications</li>
        <li class="item">Japanese</li>
      </ul>
    </header>
    <main>
      <article class="cover-article"><img class="img" src="src/image/img.png" alt="img">
        <div>
          <h1 class="heading">Kensho Hara</h1>
          <p class="description">National Institute of Advanced Industrial Science and Technology,<br>Intelligent Systems Research Institute,<br>Computer Vision Group,<br>Postdoctral Researcher<br></p>
          <p class="description">E-mail: kensho.hara [at] aist.go.jp<br>TEL: 029-861-5094</p>
        </div>
      </article>
      <article>
        <h2>News</h2>
        <ul>
          <li>(Feb. 21, 2018) <a href="https://arxiv.org/pdf/1711.09577.pdf">Our paper</a>is accepted to CVPR 2018.<a href="https://github.com/kenshohara/3D-ResNets-PyTorch">The codes and pretrained models</a>are available.</li>
          <li>(Oct. 4, 2017) Our paper is accepted to IEICE Transactions on Information and Systems.</li>
          <li>(Aug. 25, 2017) <a href="https://github.com/kenshohara/3D-ResNets">Our codes and pretrained models</a>of ICCV Workshop paper are available.</li>
          <li>(Aug. 22, 2017) <a href="https://arxiv.org/abs/1708.07632">Our paper</a>is accepted to ICCV 2017 Workshop.</li>
          <li>(Aug. 15, 2017) Our paper is accepted to ACMMM 2017 Workshop.</li>
        </ul>
      </article>
      <article>
        <h2>Profile</h2>
        <p>Bachelor's degree in Engineering, Nagoya University, 2012<br>Master's degree in Information Science, Nagoya University, 2014<br>Ph.D. degree in Information Science, Nagoya University, 2017</p>
      </article>
      <article>
        <h2>Research Projects</h2>
        <p>Human action recognition and detection in videos<br>(Under construction...)</p>
      </article>
      <article>
        <h2>Publications</h2>
        <h3>Journal Papers</h3>
        <ol>
          <li>Xueting Wang, <a class="name">Kensho Hara</a>, Yu Enokibori, Takatsugu Hirayama, Kenji Mase,<br><a class="title">"Personal Viewpoint Navigation based on Object Trajectory Distribution for Multi-view Videos"</a>,<br>IEICE Transactions on Information and Systems, Vol. E101-D, No. 1, pp. 193–204, 2018.</li>
          <li><a class="name">Kensho Hara</a>, Takatsugu Hirayama, Kenji Mase,<br><a class="title">"Vote Distribution Model for Hough-based Action Detection"</a>,<br>IEICE Transactions on Information and Systems, Vol. E99-D, No. 11, pp. 2796–2808, 2016.</li>
          <li><a class="name">Kensho Hara</a>, Takatsugu Hirayama, Kenji Mase,<br><a class="title">"Simultaneous Recognition of Human Action and its Location Estimation Based on Multi-view Hough Voting"</a>,<br>IEEJ Transactions on Electronics, Information and Systems, Vol. 134, No. 5, pp. 634–642, 2014. (IN JAPANESE)</li>
        </ol>
        <h3>International Conference</h3>
        <ol>
          <li><a class="name">Kensho Hara</a>, Hirokatsu Kataoka, Yutaka Satoh,<br><a class="title">"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"</a>,<br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. (accepted)<br><a href="https://arxiv.org/pdf/1711.09577.pdf">Paper</a>, 
            <a href="https://github.com/kenshohara/3D-ResNets-PyTorch">GitHub</a> (codes and pretrained models)
          </li>
          <li><a class="name">Kensho Hara</a>, Hirokatsu Kataoka, Yutaka Satoh,<br><a class="title">"Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"</a>,<br>ICCV Workshop on Action, Gesture, and Emotion Recognition, 2017.<br><a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w44/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.pdf">Paper</a>, 
            <a href="https://github.com/kenshohara/3D-ResNets">GitHub</a> (codes and pretrained models)
          </li>
          <li>Xueting Wang, Yu Enokibori, Takatsugu Hirayama, <a class="name">Kensho Hara</a>, Kenji Mase,<br><a class="title">"User Group based Viewpoint Recommendation using User Attributes for Multiview Videos"</a>,<br>ACM MM Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes (MUSA), 2017.</li>
          <li>Xueting Wang, <a class="name">Kensho Hara</a>, Yu Enokibori, Takatsugu Hirayama, Kenji Mase,<br><a class="title">"Personal Multi-view Viewpoint Recommendation based on Trajectory Distribution of the Viewing Target"</a>,<br>ACM Multimedia Conference (ACMMM), 2016.</li>
          <li><a class="name">Kensho Hara</a>, Kenji Mase,<br><a class="title">"Hough-based Action Detection with Time-warped Voting"</a>,<br>Asian Conference on Pattern Recognition (ACPR), 2015.</li>
          <li><a class="name">Kensho Hara</a>, Takatsugu Hirayama, Kenji Mase,<br><a class="title">"Trend-Sensitive Hough Forests for Action Detection"</a>,<br>International Conference on Image Processing (ICIP), 2014.</li>
          <li><a class="name">Kensho Hara</a>, Takatsugu Hirayama, Kenji Mase,<br><a class="title">"Simultaneous Action Recognition and Localization Based on Multi-View Hough Voting"</a>,<br>Asian Conference on Pattern Recognition (ACPR), 2013.</li>
        </ol>
      </article>
    </main>
  </body>
</html>